"""
Data models for the URL Ingestion & Embedding Pipeline
"""
from dataclasses import dataclass
from typing import Dict, Any, List, Optional
from datetime import datetime


@dataclass
class DocumentChunk:
    """Represents a segment of text content extracted from a Docusaurus page."""

    chunk_id: str  # UUID - Unique identifier for the text chunk
    content: str   # The actual text content of the chunk
    source_url: str  # Original URL where the content was found
    title: str = ""  # Title of the page where the chunk was found
    section: str = ""  # Section heading where the chunk was found
    position: int = 0  # Position of the chunk within the document
    created_at: datetime = None  # Timestamp when the chunk was created
    metadata: Dict[str, Any] = None  # Additional metadata about the chunk

    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now()
        if self.metadata is None:
            self.metadata = {}


@dataclass
class EmbeddingVector:
    """A high-dimensional vector representation of text content generated by Cohere models."""

    embedding_id: str  # UUID - Unique identifier for the embedding
    chunk_id: str  # Reference to the source document chunk
    vector: List[float]  # The embedding vector values
    model: str  # The model used to generate the embedding
    created_at: datetime = None  # Timestamp when the embedding was created
    metadata: Dict[str, Any] = None  # Additional metadata including source URL and content info

    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now()
        if self.metadata is None:
            self.metadata = {}


@dataclass
class CrawlConfiguration:
    """Settings that define which URLs to crawl, how to handle the site structure, and extraction parameters."""

    urls: List[str]  # List of URLs to crawl
    include_patterns: Optional[List[str]] = None  # URL patterns to include
    exclude_patterns: Optional[List[str]] = None  # URL patterns to exclude
    max_depth: int = 1  # Maximum depth to crawl
    delay_between_requests: float = 1.0  # Delay between requests in seconds


@dataclass
class SitemapURL:
    """A URL extracted from the sitemap.xml file that needs to be processed."""

    url: str  # The URL extracted from the sitemap
    last_modified: Optional[str] = None  # When the URL was last modified (from sitemap if available)
    change_frequency: Optional[str] = None  # How frequently the URL is expected to change (from sitemap if available)
    priority: Optional[float] = None  # Priority of the URL relative to other URLs (from sitemap if available)
    status: str = "pending"  # Processing status ("pending", "processing", "completed", "failed")
    error_message: Optional[str] = None  # Error message if processing failed


@dataclass
class SitemapIngestionResult:
    """The outcome of processing a sitemap, including success/failure counts and error details."""

    total_urls: int  # Total number of URLs extracted from the sitemap
    successful_processing: int  # Number of URLs successfully processed
    failed_processing: int  # Number of URLs that failed processing
    processed_urls: List[SitemapURL]  # List of SitemapURL objects with their status
    start_time: datetime  # When the sitemap processing started
    end_time: datetime  # When the sitemap processing completed
    total_duration: float  # Total duration of processing in seconds
    error_details: List[Dict[str, Any]]  # Details about any errors that occurred during processing


@dataclass
class BulkProcessingJob:
    """A job that manages the processing of multiple URLs extracted from the sitemap."""

    job_id: str  # Unique identifier for the processing job
    sitemap_url: str  # URL of the sitemap being processed
    status: str = "pending"  # Current status of the job ("pending", "in_progress", "completed", "failed")
    progress_percentage: float = 0.0  # Completion percentage (0.0 to 100.0)
    current_url_index: int = 0  # Index of the current URL being processed
    total_urls: int = 0  # Total number of URLs to process
    processed_count: int = 0  # Number of URLs processed so far
    successful_count: int = 0  # Number of successfully processed URLs so far
    failed_count: int = 0  # Number of failed URLs so far
    start_time: Optional[datetime] = None  # When the job started
    last_update_time: Optional[datetime] = None  # When the job status was last updated
    rate_limit_delay: float = 1.0  # Delay between requests in seconds to respect rate limits